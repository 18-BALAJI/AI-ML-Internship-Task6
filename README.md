# AI-ML-Internship-Task6: K-Nearest Neighbors (KNN) Classification ğŸŒ¸ğŸ”

## ğŸ“– Description
Day-6 Task of my **AI & ML Internship**: Implementation of the **K-Nearest Neighbors (KNN)** algorithm on the classic Iris dataset.  
This notebook demonstrates feature normalization, model training, evaluation, and decision boundary visualization.

---

## ğŸš€ Objectives
- Import and preprocess Iris dataset.  
- Normalize features for distance-based learning.  
- Train KNN classifier using scikit-learn.  
- Experiment with different values of **K**.  
- Evaluate model using accuracy, confusion matrix, and classification report.  
- Visualize decision boundaries.  

---

## ğŸ›  Tools & Libraries
- Python 3  
- Pandas, NumPy  
- Matplotlib, Seaborn  
- Scikit-learn  

---

## ğŸ“‚ Repository Contents
AI-ML-Internship-Task6/
â”‚
â”œâ”€â”€ task6.ipynb # Colab notebook with KNN implementation
â”œâ”€â”€ README.md # Documentation

---

## ğŸ“Š Dataset
- **Dataset Used**: [Iris Dataset (UCI/Kaggle)](https://www.kaggle.com/datasets/uciml/iris)  
- Target classes:  
  - `0` â†’ Setosa  
  - `1` â†’ Versicolor  
  - `2` â†’ Virginica  

---

## ğŸ” Implementation Workflow
1. Data Import & Exploration  
2. Train-Test Split & Normalization  
3. Train KNN Classifier  
4. Evaluate Performance (Accuracy, Confusion Matrix, Classification Report)  
5. Experiment with different **K values**  
6. Visualize Decision Boundaries  
7. Key Observations  

---

## ğŸ“ˆ Results (Sample)
- **Accuracy (k=5)**: ~97%  
- Best accuracy observed for **K = 5â€“7**  
- Confusion Matrix shows very few misclassifications  
- Clear decision boundaries between Iris species

---

## ğŸ“· Visual Outputs
- Pairplot of features  
- Heatmap of correlations  
- Accuracy vs K curve  
- Confusion Matrix Heatmap  
- Decision Boundary Plot  

*(Add screenshots after running notebook)*  

---

## ğŸ§¾ Key Learnings
- KNN relies on **distance metrics** â†’ normalization is essential.  
- Choosing the right **K** is critical to balance bias-variance tradeoff.  
- KNN is simple, interpretable, and effective on small datasets.  
- Performance degrades with **large datasets** (computational cost).  
- Visualizing decision boundaries helps interpret classification logic.  

---

## ğŸ‘¨â€ğŸ’» Author
**Balaji**  
B.Tech CSE (AI-ML), Parul University  
