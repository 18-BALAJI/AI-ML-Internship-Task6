# AI-ML-Internship-Task6: K-Nearest Neighbors (KNN) Classification 🌸🔍

## 📖 Description
Day-6 Task of my **AI & ML Internship**: Implementation of the **K-Nearest Neighbors (KNN)** algorithm on the classic Iris dataset.  
This notebook demonstrates feature normalization, model training, evaluation, and decision boundary visualization.

---

## 🚀 Objectives
- Import and preprocess Iris dataset.  
- Normalize features for distance-based learning.  
- Train KNN classifier using scikit-learn.  
- Experiment with different values of **K**.  
- Evaluate model using accuracy, confusion matrix, and classification report.  
- Visualize decision boundaries.  

---

## 🛠 Tools & Libraries
- Python 3  
- Pandas, NumPy  
- Matplotlib, Seaborn  
- Scikit-learn  

---

## 📂 Repository Contents
AI-ML-Internship-Task6/
│
├── task6.ipynb # Colab notebook with KNN implementation
├── README.md # Documentation

---

## 📊 Dataset
- **Dataset Used**: [Iris Dataset (UCI/Kaggle)](https://www.kaggle.com/datasets/uciml/iris)  
- Target classes:  
  - `0` → Setosa  
  - `1` → Versicolor  
  - `2` → Virginica  

---

## 🔎 Implementation Workflow
1. Data Import & Exploration  
2. Train-Test Split & Normalization  
3. Train KNN Classifier  
4. Evaluate Performance (Accuracy, Confusion Matrix, Classification Report)  
5. Experiment with different **K values**  
6. Visualize Decision Boundaries  
7. Key Observations  

---

## 📈 Results (Sample)
- **Accuracy (k=5)**: ~97%  
- Best accuracy observed for **K = 5–7**  
- Confusion Matrix shows very few misclassifications  
- Clear decision boundaries between Iris species

---

## 📷 Visual Outputs
- Pairplot of features  
- Heatmap of correlations  
- Accuracy vs K curve  
- Confusion Matrix Heatmap  
- Decision Boundary Plot  

*(Add screenshots after running notebook)*  

---

## 🧾 Key Learnings
- KNN relies on **distance metrics** → normalization is essential.  
- Choosing the right **K** is critical to balance bias-variance tradeoff.  
- KNN is simple, interpretable, and effective on small datasets.  
- Performance degrades with **large datasets** (computational cost).  
- Visualizing decision boundaries helps interpret classification logic.  

---

## 👨‍💻 Author
**Balaji**  
B.Tech CSE (AI-ML), Parul University  
